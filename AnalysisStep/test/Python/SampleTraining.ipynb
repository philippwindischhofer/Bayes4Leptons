{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.10/09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from ROOT import TFile\n",
    "from root_numpy import root2array, root2rec, tree2array\n",
    "import array\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.engine.topology import Input\n",
    "from keras.engine.training import Model\n",
    "from keras import losses, optimizers\n",
    "from keras import backend as K\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-19 19:12:52.303106: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2018-03-19 19:12:52.303134: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2018-03-19 19:12:52.303145: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(intra_op_parallelism_threads=10, inter_op_parallelism_threads=10, \\\n",
    "                        allow_soft_placement=True, device_count = {'CPU': 10})\n",
    "session = tf.Session(config = config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_size(filepath):\n",
    "    print \"skimming \" + filepath\n",
    "    f = TFile.Open(filepath)\n",
    "    size = f.Get(\"ZZTree/candTree\").GetEntries()\n",
    "    f.Close()\n",
    "\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this simulates a single ROOT file that is actually distributed over multiple \"physical\" ROOT trees. From each file in the list,\n",
    "# only the portion between start_fraction and end_fraction is considered\n",
    "class FileCollection:\n",
    "    def __init__(self, files, start_fraction, end_fraction):\n",
    "        self.files = files\n",
    "        self.start_fraction = start_fraction\n",
    "        self.end_fraction = end_fraction\n",
    "\n",
    "        # the number of entries in each file, and the local start- and endpositions\n",
    "        self.lengths = [get_size(file_path) for file_path in files]\n",
    "        self.minpos = [int(length * start_fraction) for length in self.lengths]\n",
    "        self.maxpos = [int(length * end_fraction) for length in self.lengths]\n",
    "        \n",
    "        self.used_lengths = [end - beginning for (end, beginning) in zip(self.maxpos, self.minpos)]\n",
    "                \n",
    "        self.total_length = sum(self.lengths)\n",
    "        self.used_length = sum(self.used_lengths)\n",
    "        \n",
    "        print \"collection set up: \" + str(len(files)) + \" files, \" + str(self.total_length) + \" entries in total, \" + str(self.used_length) + \" of which will be used\"\n",
    "        \n",
    "    def get_length(self):\n",
    "        return self.used_length\n",
    "    \n",
    "    # returns some data from this file collection\n",
    "    def get_data(self, branches, start_index, end_index):\n",
    "        # now need to translate between a global index, and a filepath and its corresponding local index\n",
    "        return 0\n",
    "        \n",
    "    def transform_index(self, global_index):\n",
    "        if global_index >= self.get_length():\n",
    "            raise IndexError(\"global index out of range\")\n",
    "        \n",
    "        # first determine which file in the list is needed to read this index\n",
    "        max_local_indices = np.array(self.used_lengths) - 1  # all works by 0-indexing\n",
    "        cum_lengths = np.cumsum(self.used_lengths)\n",
    "        \n",
    "        needed_file = 0\n",
    "        while global_index > cum_lengths[needed_file] - 1:\n",
    "            needed_file += 1\n",
    "            \n",
    "        # then determine the corresponding local index within this file\n",
    "        local_minpos = np.append(0, cum_lengths)\n",
    "                \n",
    "        local_index = global_index - local_minpos[needed_file]\n",
    "        \n",
    "        # up to now, all these indices are relative w.r.t. the used slice in each file. the beginning of these slices can be shifted w.r.t. the beginning of the file itself\n",
    "        local_index += self.minpos[needed_file]\n",
    "            \n",
    "        return self.files[needed_file], local_index\n",
    "    \n",
    "    def transform_index_range(self, global_start_index, global_end_index):\n",
    "        if global_start_index >= self.get_length() or global_end_index >= self.get_length():\n",
    "            raise IndexError(\"global index out of range\")\n",
    "        if global_end_index < global_start_index:\n",
    "            raise IndexError(\"end ought to come after beginning\")\n",
    "            \n",
    "        local_coords = [self.transform_index(global_index) for global_index in range(global_start_index, global_end_index)]\n",
    "        needed_files = set([local_coord[0] for local_coord in local_coords])\n",
    "        \n",
    "        retval = []\n",
    "        # now look at each needed file in turn and determine the relevant index range in this local file\n",
    "        for needed_file in needed_files:\n",
    "            needed_local_indices = [local_coord[1] for local_coord in local_coords if local_coord[0] == needed_file ]\n",
    "            needed_min_index = min(needed_local_indices)\n",
    "            needed_max_index = max(needed_local_indices)\n",
    "            \n",
    "            retval.append([needed_file, needed_min_index, needed_max_index])\n",
    "        \n",
    "        return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_training_data(H1_files, H0_files, branches, training_split = 0.5, chunks = 1000):\n",
    "    H1_collection = FileCollection(H1_files, start_fraction = 0.0, end_fraction = training_split)\n",
    "    H0_collection = FileCollection(H0_files, start_fraction = 0.0, end_fraction = training_split)\n",
    "    return datagen(H1_collection, H0_collection, branches, chunks = chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_validation_data(H1_files, H0_files, branches, training_split = 0.5, chunks = 1000):\n",
    "    H1_collection = FileCollection(H1_files, start_fraction = training_split, end_fraction = 1.0)\n",
    "    H0_collection = FileCollection(H0_files, start_fraction = training_split, end_fraction = 1.0)\n",
    "    return datagen(H1_collection, H0_collection, branches, chunks = chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(collection, start, stop, branches):\n",
    "    #print \"requesting data in range (\" + str(start) + \", \" + str(stop) + \")\"\n",
    "    filetuple = collection.transform_index_range(start, stop)\n",
    "        \n",
    "    files = [entry[0] for entry in filetuple]\n",
    "    start_indices = [entry[1] for entry in filetuple]\n",
    "    stop_indices = [entry[2] for entry in filetuple]\n",
    "    \n",
    "    read_list = []\n",
    "    for (cur_file, cur_start_index, cur_stop_index) in zip(files, start_indices, stop_indices):\n",
    "        #print \"reading from \" + cur_file + \": (\" + str(cur_start_index) + \", \" + str(cur_stop_index) + \")\" \n",
    "        read_list.append(pd.DataFrame(root2array(cur_file, treename = \"ZZTree/candTree\", branches = branches, start = cur_start_index, stop = cur_stop_index)))\n",
    "        #print \"read successful\"\n",
    "        \n",
    "    return pd.concat(read_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def datagen(H1_collection, H0_collection, branches, chunks = 100):\n",
    "    H1_curpos = 0\n",
    "    H0_curpos = 0\n",
    "\n",
    "    H1_maxpos = H1_collection.get_length()\n",
    "    H0_maxpos = H0_collection.get_length()\n",
    "\n",
    "    print \"H1 contains \" + str(H1_maxpos) + \" entries\"\n",
    "    print \"H0 contains \" + str(H0_maxpos) + \" entries\"\n",
    "\n",
    "    H1_chunksize = int(H1_maxpos / chunks)\n",
    "    H0_chunksize = int(H0_maxpos / chunks)\n",
    "    \n",
    "    print \"using the following chunk sizes: \" + \"(\" + str(H1_chunksize) + \" / \" + str(H0_chunksize) + \")\"\n",
    "    \n",
    "    # as a generator, deliver data forever\n",
    "    while True:\n",
    "               \n",
    "        if H1_curpos + H1_chunksize > H1_maxpos:\n",
    "            H1_curpos = 0\n",
    "            \n",
    "        if H0_curpos + H0_chunksize > H0_maxpos:\n",
    "            H0_curpos = 0\n",
    "        \n",
    "        # prepare next training data chunk set by drawing events randomly from the two files\n",
    "        H1_data = read_data(H1_collection, branches = branches, start = H1_curpos, stop = H1_curpos + H1_chunksize)\n",
    "\n",
    "        H0_data = read_data(H0_collection, branches = branches, start = H0_curpos, stop = H0_curpos + H0_chunksize)\n",
    "                        \n",
    "        # update the starting position for the next chunk\n",
    "        H1_curpos += H1_chunksize\n",
    "        H0_curpos += H0_chunksize\n",
    "\n",
    "        # add the truth information\n",
    "        H1_data[\"target\"] = 1.0\n",
    "        H0_data[\"target\"] = 0.0\n",
    "\n",
    "        data_chunk = pd.concat([H1_data, H0_data])\n",
    "\n",
    "        # return a randomized signal + background sample\n",
    "        training_data = data_chunk.sample(frac = 1)\n",
    "        input_data = training_data[branches].as_matrix()\n",
    "        target_data = training_data[\"target\"].as_matrix()\n",
    "        \n",
    "        yield input_data, target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inpath = \"/data_CMS/cms/wind/CJLST_NTuples/\"\n",
    "filename = \"/ZZ4lAnalysis.root\"\n",
    "H1_files = [\"VBFH125\"]\n",
    "H0_files = [\"ggH125\"]\n",
    "\n",
    "H1_paths = [inpath + H1_file + filename for H1_file in H1_files]\n",
    "H0_paths = [inpath + H0_file + filename for H0_file in H0_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "branches = [\"PFMET\", \"nCleanedJetsPt30\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "in_layer = Input(shape = (2,))\n",
    "x = Dense(128, activation = \"relu\")(in_layer)\n",
    "x = Dense(128, activation = \"relu\")(x)\n",
    "x = Dense(128, activation = \"relu\")(x)\n",
    "x = Dense(16, activation = \"relu\")(x)\n",
    "out_layer = Dense(1, activation = \"tanh\", name = \"out_layer\")(x)\n",
    "model = Model(in_layer, out_layer, name = \"testmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr = 0.1)\n",
    "model.compile(loss = \"mean_squared_error\", optimizer = sgd, metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skimming /data_CMS/cms/wind/CJLST_NTuples/VBFH125/ZZ4lAnalysis.root\n",
      "collection set up: 1 files, 62320 entries in total, 31160 of which will be used\n",
      "skimming /data_CMS/cms/wind/CJLST_NTuples/ggH125/ZZ4lAnalysis.root\n",
      "collection set up: 1 files, 110483 entries in total, 55241 of which will be used\n",
      "skimming /data_CMS/cms/wind/CJLST_NTuples/VBFH125/ZZ4lAnalysis.root\n",
      "collection set up: 1 files, 62320 entries in total, 31160 of which will be used\n",
      "skimming /data_CMS/cms/wind/CJLST_NTuples/ggH125/ZZ4lAnalysis.root\n",
      "collection set up: 1 files, 110483 entries in total, 55242 of which will be used\n"
     ]
    }
   ],
   "source": [
    "train_gen = generate_training_data(H1_paths, H0_paths, branches, chunks = 100)\n",
    "val_gen = generate_validation_data(H1_paths, H0_paths, branches, chunks = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor = 'val_loss',\n",
    "                          patience = 10,\n",
    "                          verbose = 1,\n",
    "                          mode = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1 contains 31160 entriesEpoch 1/50\n",
      "H0 contains 55241 entries\n",
      "\n",
      "using the following chunk sizes: (311 / 552)\n",
      "H1 contains 31160 entries\n",
      "H0 contains 55242 entries\n",
      "using the following chunk sizes: (311 / 552)\n",
      "11s - loss: 0.3008 - acc: 0.6044 - val_loss: 0.2304 - val_acc: 0.6400\n",
      "Epoch 2/50\n",
      "11s - loss: 0.2304 - acc: 0.6400 - val_loss: 0.2304 - val_acc: 0.6400\n",
      "Epoch 3/50\n",
      "10s - loss: 0.2304 - acc: 0.6400 - val_loss: 0.2304 - val_acc: 0.6400\n",
      "Epoch 4/50\n",
      "10s - loss: 0.2304 - acc: 0.6400 - val_loss: 0.2304 - val_acc: 0.6400\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2b099db1581b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/py2-pippkgs_depscipy/3.0-fmblme3/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/py2-pippkgs_depscipy/3.0-fmblme3/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1900\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1901\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1902\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/py2-pippkgs_depscipy/3.0-fmblme3/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1640\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1642\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1643\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/py2-pippkgs_depscipy/3.0-fmblme3/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/py2-pippkgs_depscipy/3.0-fmblme3/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/py2-pippkgs_depscipy/3.0-fmblme3/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/py2-pippkgs_depscipy/3.0-fmblme3/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/py2-pippkgs_depscipy/3.0-fmblme3/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/py2-pippkgs_depscipy/3.0-fmblme3/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ret = model.fit_generator(train_gen, steps_per_epoch = 128, epochs = 50, verbose = 2, validation_data = val_gen, validation_steps = 10, callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
