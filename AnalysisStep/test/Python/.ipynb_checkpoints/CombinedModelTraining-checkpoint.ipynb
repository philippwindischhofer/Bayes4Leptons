{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.10/09\n"
     ]
    }
   ],
   "source": [
    "from trainlib.CombinedPreprocessor import CombinedPreprocessor\n",
    "from trainlib.PCAWhiteningPreprocessor import PCAWhiteningPreprocessor\n",
    "from trainlib.RNNPreprocessor import RNNPreprocessor\n",
    "from trainlib.generator import Generator\n",
    "import trainlib.cuts as cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, LSTM, Activation\n",
    "from keras.engine.topology import Input\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.engine.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-29 12:17:41.423648: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2018-03-29 12:17:41.423683: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2018-03-29 12:17:41.423691: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(intra_op_parallelism_threads = 10, inter_op_parallelism_threads = 10, allow_soft_placement = True, device_count = {'CPU': 10})\n",
    "session = tf.Session(config = config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA setup: ['PFMET', 'nCleanedJetsPt30']\n",
      "for list input group 'Jet': assigned periodic inputs ['JetPhi'] and nonperiodic inputs ['JetPt', 'JetEta'], sorting according to JetPt\n",
      "PCA setup: ['JetPt', 'JetEta', 'JetPhi_sin', 'JetPhi_cos']\n",
      "total processed columns: ['JetPt', 'JetEta', 'JetPhi', 'PFMET', 'nCleanedJetsPt30']\n"
     ]
    }
   ],
   "source": [
    "scalar_inputs = [\"PFMET\", \"nCleanedJetsPt30\"]\n",
    "list_inputs = {'Jet': [\"JetPt\", \"JetEta\", \"JetPhi\"]}\n",
    "\n",
    "pre = CombinedPreprocessor('combined_test', scalar_inputs, PCAWhiteningPreprocessor, list_inputs, RNNPreprocessor, cuts.no_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skimming /data_CMS/cms/wind/CJLST_NTuples/ggH125/ZZ4lAnalysis.root\n",
      "collection set up: 1 files, 110483 entries in total, 55241 of which will be used\n",
      "skimming /data_CMS/cms/wind/CJLST_NTuples/VBFH125/ZZ4lAnalysis.root\n",
      "collection set up: 1 files, 62320 entries in total, 31160 of which will be used\n",
      "H1 contains 55241 entries\n",
      "H0 contains 31160 entries\n",
      "using the following chunk sizes: (55 / 31)\n",
      "setting up preprocessor on 86430 events\n",
      "Index([u'JetPt', u'JetEta', u'JetPhi', u'PFMET', u'nCleanedJetsPt30'], dtype='object')\n",
      "setting up scalar preprocessor\n",
      "setting up PCA whitening on 86430 events\n",
      "86430 remaining after the cuts\n",
      "setting up list preprocessor for 'Jet'\n",
      "List: Index([u'JetPt', u'JetEta', u'JetPhi_sin', u'JetPhi_cos'], dtype='object')\n",
      "86430 remaining after the cuts\n",
      "setting up PCA whitening on 102669 events\n",
      "102669 remaining after the cuts\n",
      "found a maximum list length in the setup data of 9: will pad or truncate to this length from now on\n"
     ]
    }
   ],
   "source": [
    "H1_stream = {\"/data_CMS/cms/wind/CJLST_NTuples/ggH125/ZZ4lAnalysis.root\" : cuts.no_cut}\n",
    "H0_stream = {\"/data_CMS/cms/wind/CJLST_NTuples/VBFH125/ZZ4lAnalysis.root\" : cuts.no_cut}\n",
    "\n",
    "testgen = Generator(H1_stream, H0_stream, pre.processed_columns, preprocessor = None)\n",
    "setup_len = testgen.setup_training_data()\n",
    "pre.setup_generator(testgen.raw_generator_scrambled(), len_setupdata = setup_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CombinedModel:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        \n",
    "    def build(self):\n",
    "        in_layers_lstm = []\n",
    "        out_layers_lstm = []\n",
    "        \n",
    "        for i in range(3):\n",
    "            in_layer_lstm = Input(shape = (None, 4), name = 'Jet')\n",
    "            # number units = dimensionality of the output space\n",
    "            lstm = LSTM(units = 16, return_sequences = False)(in_layer_lstm)\n",
    "            out_layer_lstm = Dense(9, activation = 'relu')(lstm)\n",
    "            \n",
    "            print out_layer_lstm\n",
    "            \n",
    "            out_layers_lstm.append(out_layer_lstm)\n",
    "            in_layers_lstm.append(in_layer_lstm)\n",
    "        \n",
    "        in_layer_dense = Input(shape = (2,), name = 'scalar_inputs')\n",
    "        \n",
    "        print out_layers_lstm\n",
    "        \n",
    "        out_layers_lstm.append(in_layer_dense)\n",
    "        \n",
    "        print out_layers_lstm\n",
    "        \n",
    "        pcl = keras.layers.concatenate(out_layers_lstm)\n",
    "\n",
    "        print pcl\n",
    "        \n",
    "        #input_layer_dense = in_layers_lstm[0] + [in_layer_dense]\n",
    "                \n",
    "        x = keras.layers.concatenate(input_layer_dense)\n",
    "        x = Dense(128, activation = 'tanh')(x)\n",
    "        x = Dense(128, activation = 'tanh')(x)\n",
    "\n",
    "        out_layer = Dense(1, activation = 'tanh', name = 'target')(x)\n",
    "        \n",
    "        self.model = keras.engine.training.Model(inputs = [in_layer_lstm, in_layer_dense], outputs = [out_layer], name = 'combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense_48/Relu:0\", shape=(?, 9), dtype=float32)\n",
      "Tensor(\"dense_49/Relu:0\", shape=(?, 9), dtype=float32)\n",
      "Tensor(\"dense_50/Relu:0\", shape=(?, 9), dtype=float32)\n",
      "[<tf.Tensor 'dense_48/Relu:0' shape=(?, 9) dtype=float32>, <tf.Tensor 'dense_49/Relu:0' shape=(?, 9) dtype=float32>, <tf.Tensor 'dense_50/Relu:0' shape=(?, 9) dtype=float32>]\n",
      "[<tf.Tensor 'dense_48/Relu:0' shape=(?, 9) dtype=float32>, <tf.Tensor 'dense_49/Relu:0' shape=(?, 9) dtype=float32>, <tf.Tensor 'dense_50/Relu:0' shape=(?, 9) dtype=float32>, <tf.Tensor 'scalar_inputs_15:0' shape=(?, 2) dtype=float32>]\n",
      "Tensor(\"concatenate_12/concat:0\", shape=(?, 29), dtype=float32)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'input_layer_dense' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-6c1ee84f330a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCombinedModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"mean_squared_error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-ba8e3fd6019f>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m#input_layer_dense = in_layers_lstm[0] + [in_layer_dense]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_layer_dense\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tanh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tanh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'input_layer_dense' is not defined"
     ]
    }
   ],
   "source": [
    "mod = CombinedModel()\n",
    "mod.build()\n",
    "sgd = optimizers.SGD(lr = 0.01, momentum = 0.9)\n",
    "mod.model.compile(loss = \"mean_squared_error\", optimizer = sgd, metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skimming /data_CMS/cms/wind/CJLST_NTuples/ggH125/ZZ4lAnalysis.root\n",
      "collection set up: 1 files, 110483 entries in total, 55241 of which will be used\n",
      "skimming /data_CMS/cms/wind/CJLST_NTuples/VBFH125/ZZ4lAnalysis.root\n",
      "collection set up: 1 files, 62320 entries in total, 31160 of which will be used\n",
      "skimming /data_CMS/cms/wind/CJLST_NTuples/ggH125/ZZ4lAnalysis.root\n",
      "collection set up: 1 files, 110483 entries in total, 55242 of which will be used\n",
      "skimming /data_CMS/cms/wind/CJLST_NTuples/VBFH125/ZZ4lAnalysis.root\n",
      "collection set up: 1 files, 62320 entries in total, 31160 of which will be used\n"
     ]
    }
   ],
   "source": [
    "train_gen = Generator(H1_stream, H0_stream, pre.processed_columns, preprocessor = pre.process)\n",
    "training_len = train_gen.setup_training_data()\n",
    "val_gen = Generator(H1_stream, H0_stream, pre.processed_columns, preprocessor = pre.process)\n",
    "validation_len = val_gen.setup_validation_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1 contains 55241 entriesEpoch 1/5\n",
      "\n",
      "H0 contains 31160 entries\n",
      "using the following chunk sizes: (55 / 31)\n",
      "H1 contains 55242 entries\n",
      "H0 contains 31160 entries\n",
      "using the following chunk sizes: (55 / 31)\n",
      "98s - loss: 0.2132 - acc: 0.7079 - val_loss: 0.1826 - val_acc: 0.7290\n",
      "Epoch 2/5\n",
      "93s - loss: 0.1753 - acc: 0.7339 - val_loss: 0.1731 - val_acc: 0.7354\n",
      "Epoch 3/5\n",
      "94s - loss: 0.1720 - acc: 0.7369 - val_loss: 0.1730 - val_acc: 0.7297\n",
      "Epoch 4/5\n",
      "94s - loss: 0.1754 - acc: 0.7296 - val_loss: 0.1709 - val_acc: 0.7296\n",
      "Epoch 5/5\n",
      "94s - loss: 0.1699 - acc: 0.7395 - val_loss: 0.1671 - val_acc: 0.7412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3f59faedd0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.model.fit_generator(train_gen.preprocessed_generator(), steps_per_epoch = 128, epochs = 5, \n",
    "                        verbose = 2, validation_data = val_gen.preprocessed_generator(), validation_steps = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
