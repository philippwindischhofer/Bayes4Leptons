{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.10/09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from ROOT import TFile\n",
    "from root_numpy import root2array, root2rec, tree2array\n",
    "import array\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.engine.topology import Input\n",
    "from keras.engine.training import Model\n",
    "from keras import losses, optimizers\n",
    "from keras import backend as K\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-19 18:52:03.680797: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2018-03-19 18:52:03.680833: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2018-03-19 18:52:03.680842: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(intra_op_parallelism_threads=10, inter_op_parallelism_threads=10, \\\n",
    "                        allow_soft_placement=True, device_count = {'CPU': 10})\n",
    "session = tf.Session(config = config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_size(filepath):\n",
    "    print \"skimming \" + filepath\n",
    "    f = TFile.Open(filepath)\n",
    "    size = f.Get(\"ZZTree/candTree\").GetEntries()\n",
    "    f.Close()\n",
    "\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this simulates a single ROOT file that is actually distributed over multiple \"physical\" ROOT trees. From each file in the list,\n",
    "# only the portion between start_fraction and end_fraction is considered\n",
    "class FileCollection:\n",
    "    def __init__(self, files, start_fraction, end_fraction):\n",
    "        self.files = files\n",
    "        self.start_fraction = start_fraction\n",
    "        self.end_fraction = end_fraction\n",
    "\n",
    "        # the number of entries in each file, and the local start- and endpositions\n",
    "        self.lengths = [get_size(file_path) for file_path in files]\n",
    "        self.minpos = [int(length * start_fraction) for length in self.lengths]\n",
    "        self.maxpos = [int(length * end_fraction) for length in self.lengths]\n",
    "        \n",
    "        self.used_lengths = [end - beginning for (end, beginning) in zip(self.maxpos, self.minpos)]\n",
    "                \n",
    "        self.total_length = sum(self.lengths)\n",
    "        self.used_length = sum(self.used_lengths)\n",
    "        \n",
    "        print \"collection set up: \" + str(len(files)) + \" files, \" + str(self.total_length) + \" entries in total, \" + str(self.used_length) + \" of which will be used\"\n",
    "        \n",
    "    def get_length(self):\n",
    "        return self.used_length\n",
    "    \n",
    "    # returns some data from this file collection\n",
    "    def get_data(self, branches, start_index, end_index):\n",
    "        # now need to translate between a global index, and a filepath and its corresponding local index\n",
    "        return 0\n",
    "        \n",
    "    def transform_index(self, global_index):\n",
    "        if global_index >= self.get_length():\n",
    "            raise IndexError(\"global index out of range\")\n",
    "        \n",
    "        # first determine which file in the list is needed to read this index\n",
    "        max_local_indices = np.array(self.used_lengths) - 1  # all works by 0-indexing\n",
    "        cum_lengths = np.cumsum(self.used_lengths)\n",
    "        \n",
    "        needed_file = 0\n",
    "        while global_index > cum_lengths[needed_file] - 1:\n",
    "            needed_file += 1\n",
    "            \n",
    "        # then determine the corresponding local index within this file\n",
    "        local_minpos = np.append(0, cum_lengths)\n",
    "                \n",
    "        local_index = global_index - local_minpos[needed_file]\n",
    "        \n",
    "        # up to now, all these indices are relative w.r.t. the used slice in each file. the beginning of these slices can be shifted w.r.t. the beginning of the file itself\n",
    "        local_index += self.minpos[needed_file]\n",
    "            \n",
    "        return self.files[needed_file], local_index\n",
    "    \n",
    "    def transform_index_range(self, global_start_index, global_end_index):\n",
    "        if global_start_index >= self.get_length() or global_end_index >= self.get_length():\n",
    "            raise IndexError(\"global index out of range\")\n",
    "        if global_end_index < global_start_index:\n",
    "            raise IndexError(\"end ought to come after beginning\")\n",
    "            \n",
    "        local_coords = [self.transform_index(global_index) for global_index in range(global_start_index, global_end_index)]\n",
    "        needed_files = set([local_coord[0] for local_coord in local_coords])\n",
    "        \n",
    "        retval = []\n",
    "        # now look at each needed file in turn and determine the relevant index range in this local file\n",
    "        for needed_file in needed_files:\n",
    "            needed_local_indices = [local_coord[1] for local_coord in local_coords if local_coord[0] == needed_file ]\n",
    "            needed_min_index = min(needed_local_indices)\n",
    "            needed_max_index = max(needed_local_indices)\n",
    "            \n",
    "            retval.append([needed_file, needed_min_index, needed_max_index])\n",
    "        \n",
    "        return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_training_data(H1_files, H0_files, branches, training_split = 0.5, chunks = 1000):\n",
    "    H1_collection = FileCollection(H1_files, start_fraction = 0.0, end_fraction = training_split)\n",
    "    H0_collection = FileCollection(H0_files, start_fraction = 0.0, end_fraction = training_split)\n",
    "    return datagen(H1_collection, H0_collection, branches, chunks = chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_validation_data(H1_files, H0_files, branches, training_split = 0.5, chunks = 1000):\n",
    "    H1_collection = FileCollection(H1_files, start_fraction = training_split, end_fraction = 1.0)\n",
    "    H0_collection = FileCollection(H0_files, start_fraction = training_split, end_fraction = 1.0)\n",
    "    return datagen(H1_collection, H0_collection, branches, chunks = chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(collection, start, stop, branches):\n",
    "    #print \"requesting data in range (\" + str(start) + \", \" + str(stop) + \")\"\n",
    "    filetuple = collection.transform_index_range(start, stop)\n",
    "        \n",
    "    files = [entry[0] for entry in filetuple]\n",
    "    start_indices = [entry[1] for entry in filetuple]\n",
    "    stop_indices = [entry[2] for entry in filetuple]\n",
    "    \n",
    "    read_list = []\n",
    "    for (cur_file, cur_start_index, cur_stop_index) in zip(files, start_indices, stop_indices):\n",
    "        #print \"reading from \" + cur_file + \": (\" + str(cur_start_index) + \", \" + str(cur_stop_index) + \")\" \n",
    "        read_list.append(pd.DataFrame(root2array(cur_file, treename = \"ZZTree/candTree\", branches = branches, start = cur_start_index, stop = cur_stop_index)))\n",
    "        #print \"read successful\"\n",
    "        \n",
    "    return pd.concat(read_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def datagen(H1_collection, H0_collection, branches, chunks = 100):\n",
    "    H1_curpos = 0\n",
    "    H0_curpos = 0\n",
    "\n",
    "    H1_maxpos = H1_collection.get_length()\n",
    "    H0_maxpos = H0_collection.get_length()\n",
    "\n",
    "    print \"H1 contains \" + str(H1_maxpos) + \" entries\"\n",
    "    print \"H0 contains \" + str(H0_maxpos) + \" entries\"\n",
    "\n",
    "    H1_chunksize = int(H1_maxpos / chunks)\n",
    "    H0_chunksize = int(H0_maxpos / chunks)\n",
    "    \n",
    "    print \"using the following chunk sizes: \" + \"(\" + str(H1_chunksize) + \" / \" + str(H0_chunksize) + \")\"\n",
    "    \n",
    "    # as a generator, deliver data forever\n",
    "    while True:\n",
    "               \n",
    "        if H1_curpos + H1_chunksize > H1_maxpos:\n",
    "            H1_curpos = 0\n",
    "            \n",
    "        if H0_curpos + H0_chunksize > H0_maxpos:\n",
    "            H0_curpos = 0\n",
    "        \n",
    "        # prepare next training data chunk set by drawing events randomly from the two files\n",
    "        H1_data = read_data(H1_collection, branches = branches, start = H1_curpos, stop = H1_curpos + H1_chunksize)\n",
    "\n",
    "        H0_data = read_data(H0_collection, branches = branches, start = H0_curpos, stop = H0_curpos + H0_chunksize)\n",
    "                        \n",
    "        # update the starting position for the next chunk\n",
    "        H1_curpos += H1_chunksize\n",
    "        H0_curpos += H0_chunksize\n",
    "\n",
    "        # add the truth information\n",
    "        H1_data[\"target\"] = 1.0\n",
    "        H0_data[\"target\"] = 0.0\n",
    "\n",
    "        data_chunk = pd.concat([H1_data, H0_data])\n",
    "\n",
    "        # return a randomized signal + background sample\n",
    "        training_data = data_chunk.sample(frac = 1)\n",
    "        input_data = training_data[branches].as_matrix()\n",
    "        target_data = training_data[\"target\"].as_matrix()\n",
    "        \n",
    "        yield input_data, target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inpath = \"/data_CMS/cms/wind/CJLST_NTuples/\"\n",
    "filename = \"/ZZ4lAnalysis.root\"\n",
    "H1_files = [\"VBFH125\"]\n",
    "H0_files = [\"ggH125\"]\n",
    "\n",
    "H1_paths = [inpath + H1_file + filename for H1_file in H1_files]\n",
    "H0_paths = [inpath + H0_file + filename for H0_file in H0_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "branches = [\"PFMET\", \"nCleanedJetsPt30\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "in_layer = Input(shape = (2,))\n",
    "x = Dense(128, activation = \"relu\")(in_layer)\n",
    "x = Dense(128, activation = \"relu\")(x)\n",
    "x = Dense(128, activation = \"relu\")(x)\n",
    "x = Dense(16, activation = \"relu\")(x)\n",
    "out_layer = Dense(1, activation = \"tanh\", name = \"out_layer\")(x)\n",
    "model = Model(in_layer, out_layer, name = \"testmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr = 0.1)\n",
    "model.compile(loss = \"mean_squared_error\", optimizer = sgd, metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skimming /data_CMS/cms/wind/CJLST_NTuples/VBFH125/ZZ4lAnalysis.root\n",
      "collection set up: 1 files, 62320 entries in total, 31160 of which will be used\n",
      "skimming /data_CMS/cms/wind/CJLST_NTuples/ggH125/ZZ4lAnalysis.root\n",
      "collection set up: 1 files, 110483 entries in total, 55241 of which will be used\n",
      "skimming /data_CMS/cms/wind/CJLST_NTuples/VBFH125/ZZ4lAnalysis.root\n",
      "collection set up: 1 files, 62320 entries in total, 31160 of which will be used\n",
      "skimming /data_CMS/cms/wind/CJLST_NTuples/ggH125/ZZ4lAnalysis.root\n",
      "collection set up: 1 files, 110483 entries in total, 55242 of which will be used\n"
     ]
    }
   ],
   "source": [
    "train_gen = generate_training_data(H1_paths, H0_paths, branches, chunks = 100)\n",
    "val_gen = generate_validation_data(H1_paths, H0_paths, branches, chunks = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor = 'val_loss',\n",
    "                          patience = 10,\n",
    "                          verbose = 1,\n",
    "                          mode = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1 contains 31160 entriesEpoch 1/50\n",
      "\n",
      "H0 contains 55241 entries\n",
      "using the following chunk sizes: (311 / 552)\n",
      "H1 contains 31160 entries\n",
      "H0 contains 55242 entries\n",
      "using the following chunk sizes: (311 / 552)\n",
      "11s - loss: 0.2852 - acc: 0.6077 - val_loss: 0.2278 - val_acc: 0.6423\n",
      "Epoch 2/50\n",
      "10s - loss: 0.2266 - acc: 0.6395 - val_loss: 0.2060 - val_acc: 0.6616\n",
      "Epoch 3/50\n",
      "10s - loss: 0.2238 - acc: 0.6490 - val_loss: 0.2279 - val_acc: 0.6411\n",
      "Epoch 4/50\n",
      "10s - loss: 0.2269 - acc: 0.6456 - val_loss: 0.2283 - val_acc: 0.6447\n",
      "Epoch 5/50\n",
      "10s - loss: 0.2277 - acc: 0.6498 - val_loss: 0.2317 - val_acc: 0.6412\n",
      "Epoch 6/50\n",
      "10s - loss: 0.2215 - acc: 0.6542 - val_loss: 0.2087 - val_acc: 0.6416\n",
      "Epoch 7/50\n",
      "10s - loss: 0.2094 - acc: 0.6545 - val_loss: 0.1914 - val_acc: 0.6575\n",
      "Epoch 8/50\n",
      "10s - loss: 0.2044 - acc: 0.6564 - val_loss: 0.1882 - val_acc: 0.6516\n",
      "Epoch 9/50\n",
      "10s - loss: 0.1996 - acc: 0.6691 - val_loss: 0.1950 - val_acc: 0.6485\n",
      "Epoch 10/50\n",
      "11s - loss: 0.1980 - acc: 0.6741 - val_loss: 0.1944 - val_acc: 0.6513\n",
      "Epoch 11/50\n",
      "11s - loss: 0.1955 - acc: 0.6930 - val_loss: 0.1809 - val_acc: 0.7298\n",
      "Epoch 12/50\n",
      "10s - loss: 0.1954 - acc: 0.7013 - val_loss: 0.1789 - val_acc: 0.7262\n",
      "Epoch 13/50\n",
      "10s - loss: 0.1902 - acc: 0.7129 - val_loss: 0.2115 - val_acc: 0.6728\n",
      "Epoch 14/50\n",
      "10s - loss: 0.1887 - acc: 0.7144 - val_loss: 0.1916 - val_acc: 0.7153\n",
      "Epoch 15/50\n",
      "10s - loss: 0.1828 - acc: 0.7238 - val_loss: 0.1679 - val_acc: 0.7374\n",
      "Epoch 16/50\n",
      "10s - loss: 0.1808 - acc: 0.7231 - val_loss: 0.1776 - val_acc: 0.7338\n",
      "Epoch 17/50\n",
      "10s - loss: 0.1825 - acc: 0.7237 - val_loss: 0.1767 - val_acc: 0.7242\n",
      "Epoch 18/50\n",
      "10s - loss: 0.1801 - acc: 0.7254 - val_loss: 0.1736 - val_acc: 0.7261\n",
      "Epoch 19/50\n",
      "10s - loss: 0.1765 - acc: 0.7286 - val_loss: 0.1749 - val_acc: 0.7256\n",
      "Epoch 20/50\n",
      "11s - loss: 0.1772 - acc: 0.7296 - val_loss: 0.1740 - val_acc: 0.7315\n",
      "Epoch 21/50\n",
      "11s - loss: 0.1780 - acc: 0.7285 - val_loss: 0.1702 - val_acc: 0.7408\n",
      "Epoch 22/50\n",
      "11s - loss: 0.1758 - acc: 0.7313 - val_loss: 0.1727 - val_acc: 0.7315\n",
      "Epoch 23/50\n",
      "11s - loss: 0.1754 - acc: 0.7298 - val_loss: 0.1730 - val_acc: 0.7289\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2b099db1581b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/py2-pippkgs_depscipy/3.0-fmblme3/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/py2-pippkgs_depscipy/3.0-fmblme3/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1869\u001b[0m                             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1870\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1871\u001b[0;31m                             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1873\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ret = model.fit_generator(train_gen, steps_per_epoch = 128, epochs = 50, verbose = 2, validation_data = val_gen, validation_steps = 10, callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
