{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import subprocess as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from trainlib.ConfigFileHandler import ConfigFileHandler\n",
    "from trainlib.ConfigFileUtils import ConfigFileUtils\n",
    "from trainlib.ModelCollectionConfigFileHandler import ModelCollectionConfigFileHandler\n",
    "from DistributeTrainingSettings import distribute_training_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this looks through several training runs (e.g. as they originate from a hyperparameter sweep) and takes for each \n",
    "# model the best one that was encountered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss(run, mcoll, model):\n",
    "    confhandler = ConfigFileHandler()\n",
    "    confhandler.load_configuration(os.path.join(campaign_workspace, run, \"training\", mcoll, \"model_benchmark.txt\"))\n",
    "    return float(confhandler.get_field(model, 'val_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# directory that holds all the different hyperparameter training campaigns\n",
    "campaign_workspace = \"/path/to/campaign_rootdir/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# runs to check for (good) models (the first one passed is taken as reference run from which the available models\n",
    "# are taken - it is expected that all others runs also follow this structure):\n",
    "input_runs = [\"campaign_1/run_1/\",\n",
    "              \"campaign_1/run_2/\",\n",
    "              \" .... \"\n",
    "              \"campaign_2/run_1/\",\n",
    "              \"campaign_2/run_2/\",\n",
    "              \" .... \"\n",
    "              \"campaign_n/run_m\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output training campaign, this will consist of a combination of the models found in the campaigns listed above, in such a way that the overall performance is optimized\n",
    "output_run = \"/path/to/output_campaign\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# where the configuration file for the hyperparameter settings should be stored\n",
    "hyperparam_output = \"/path/to/hyperparameter.conf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(campaign_workspace, output_run))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the available model names\n",
    "reference_run = input_runs[0]\n",
    "available_mcolls = os.walk(os.path.join(campaign_workspace, reference_run, \"training\")).next()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mcolls_winning = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for mcoll in available_mcolls:\n",
    "    models = os.walk(os.path.join(campaign_workspace, reference_run, \"training\", mcoll)).next()[1]\n",
    "\n",
    "    # load a representative version of the current model collection...\n",
    "    mconfhandler = ModelCollectionConfigFileHandler()\n",
    "    mconfhandler.load_configuration(os.path.join(campaign_workspace, reference_run, \"settings_training\", mcoll, \"settings.conf\"))\n",
    "    mcoll_template = mconfhandler.GetModelCollection()[0]\n",
    "    \n",
    "    # ... but strip away all the actual model components\n",
    "    mcoll_template.model_dict = {}\n",
    "    mcoll_template.preprocessor_dict = {}\n",
    "    mcoll_template.settings_dict = {}\n",
    "    \n",
    "    for model in models:\n",
    "        # compare this model across the different runs\n",
    "        losses = [get_loss(run, mcoll, model) for run in input_runs]\n",
    "\n",
    "        winner = np.argmin(losses)\n",
    "        \n",
    "        winning_run = input_runs[winner]\n",
    "        \n",
    "        # copy the winning model into the output run\n",
    "        shutil.copytree(os.path.join(campaign_workspace, winning_run, \"training\", mcoll, model),\n",
    "                        os.path.join(campaign_workspace, output_run, \"training\", mcoll, model))\n",
    "\n",
    "        print \"--------------------------------------------\"\n",
    "        print \" take \" + model + \" from \" + winning_run\n",
    "        print \"--------------------------------------------\"\n",
    "        \n",
    "        # load the winning model to keep track of its settings\n",
    "        mconfhandler = ModelCollectionConfigFileHandler()\n",
    "        mconfhandler.load_configuration(os.path.join(campaign_workspace, winning_run, \"settings_training\", mcoll, \"settings.conf\"))\n",
    "        mcoll_winning = mconfhandler.GetModelCollection()[0]\n",
    "        \n",
    "        # then pull the winning model over into the template\n",
    "        winning_model = mcoll_winning.model_dict[model]\n",
    "        winning_preprocessor = mcoll_winning.preprocessor_dict[model]\n",
    "        winning_settings = mcoll_winning.settings_dict[model]\n",
    "        \n",
    "        mcoll_template.add_model(winning_preprocessor, winning_model, winning_settings)\n",
    "        \n",
    "    mcolls_winning.append(mcoll_template)\n",
    "    \n",
    "# now save the put-together config file also into the output run\n",
    "mconfhandler = ModelCollectionConfigFileHandler()\n",
    "mconfhandler.ToConfiguration(mcolls_winning)\n",
    "mconfhandler.save_configuration(os.path.join(campaign_workspace, output_run, \"settings.conf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now distriute again the training settings, as usual:\n",
    "distribute_training_settings(os.path.join(campaign_workspace, output_run + '/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now create the hyperparameter config file for each model, taken from the winners\n",
    "hp_confhandler = ConfigFileHandler()\n",
    "for mcoll in mcolls_winning:\n",
    "    for model_name, model in mcoll.model_dict.iteritems():\n",
    "        hp_confhandler.new_section(model_name)\n",
    "        hp_confhandler.set_field(model_name, \"hyperparameters\", ConfigFileUtils.serialize_dict(model.hyperparameters, lambda x: str(x)))\n",
    "        \n",
    "hp_confhandler.save_configuration(hyperparam_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
